{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking and selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll exemplify some of scikit-learn's ranking functions used to score the importance of features. We'll reuse the running example, the Adult dataset that we used in the first exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'capital-gain', 'capital-loss', 'education-num',\n",
       "       'education=10th', 'education=11th', 'education=12th',\n",
       "       'education=1st-4th', 'education=5th-6th', 'education=7th-8th',\n",
       "       'education=9th', 'education=Assoc-acdm', 'education=Assoc-voc',\n",
       "       'education=Bachelors', 'education=Doctorate', 'education=HS-grad',\n",
       "       'education=Masters', 'education=Preschool',\n",
       "       'education=Prof-school', 'education=Some-college',\n",
       "       'hours-per-week', 'marital-status=Divorced',\n",
       "       'marital-status=Married-AF-spouse',\n",
       "       'marital-status=Married-civ-spouse',\n",
       "       'marital-status=Married-spouse-absent',\n",
       "       'marital-status=Never-married', 'marital-status=Separated',\n",
       "       'marital-status=Widowed', 'native-country=?',\n",
       "       'native-country=Cambodia', 'native-country=Canada',\n",
       "       'native-country=China', 'native-country=Columbia',\n",
       "       'native-country=Cuba', 'native-country=Dominican-Republic',\n",
       "       'native-country=Ecuador', 'native-country=El-Salvador',\n",
       "       'native-country=England', 'native-country=France',\n",
       "       'native-country=Germany', 'native-country=Greece',\n",
       "       'native-country=Guatemala', 'native-country=Haiti',\n",
       "       'native-country=Holand-Netherlands', 'native-country=Honduras',\n",
       "       'native-country=Hong', 'native-country=Hungary',\n",
       "       'native-country=India', 'native-country=Iran',\n",
       "       'native-country=Ireland', 'native-country=Italy',\n",
       "       'native-country=Jamaica', 'native-country=Japan',\n",
       "       'native-country=Laos', 'native-country=Mexico',\n",
       "       'native-country=Nicaragua',\n",
       "       'native-country=Outlying-US(Guam-USVI-etc)', 'native-country=Peru',\n",
       "       'native-country=Philippines', 'native-country=Poland',\n",
       "       'native-country=Portugal', 'native-country=Puerto-Rico',\n",
       "       'native-country=Scotland', 'native-country=South',\n",
       "       'native-country=Taiwan', 'native-country=Thailand',\n",
       "       'native-country=Trinadad&Tobago', 'native-country=United-States',\n",
       "       'native-country=Vietnam', 'native-country=Yugoslavia',\n",
       "       'occupation=?', 'occupation=Adm-clerical',\n",
       "       'occupation=Armed-Forces', 'occupation=Craft-repair',\n",
       "       'occupation=Exec-managerial', 'occupation=Farming-fishing',\n",
       "       'occupation=Handlers-cleaners', 'occupation=Machine-op-inspct',\n",
       "       'occupation=Other-service', 'occupation=Priv-house-serv',\n",
       "       'occupation=Prof-specialty', 'occupation=Protective-serv',\n",
       "       'occupation=Sales', 'occupation=Tech-support',\n",
       "       'occupation=Transport-moving', 'race=Amer-Indian-Eskimo',\n",
       "       'race=Asian-Pac-Islander', 'race=Black', 'race=Other',\n",
       "       'race=White', 'relationship=Husband', 'relationship=Not-in-family',\n",
       "       'relationship=Other-relative', 'relationship=Own-child',\n",
       "       'relationship=Unmarried', 'relationship=Wife', 'sex=Female',\n",
       "       'sex=Male', 'workclass=?', 'workclass=Federal-gov',\n",
       "       'workclass=Local-gov', 'workclass=Never-worked',\n",
       "       'workclass=Private', 'workclass=Self-emp-inc',\n",
       "       'workclass=Self-emp-not-inc', 'workclass=State-gov',\n",
       "       'workclass=Without-pay'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "train_data = pd.read_csv('Lec2_adult_train.csv')\n",
    "n_cols = len(train_data.columns)\n",
    "Xtrain_dicts = train_data.iloc[:, :n_cols-1].to_dict('records')\n",
    "Ytrain = train_data.iloc[:, n_cols-1]\n",
    "\n",
    "test_data = pd.read_csv('Lec2_adult_test.csv')\n",
    "Xtest_dicts = test_data.iloc[:, :n_cols-1].to_dict('records')\n",
    "Ytest = test_data.iloc[:, n_cols-1]\n",
    "\n",
    "dv = DictVectorizer()\n",
    "dv.fit(Xtrain_dicts)\n",
    "\n",
    "X_vec = dv.transform(Xtrain_dicts)\n",
    "\n",
    "dv.get_feature_names_out()\n",
    "\n",
    "#feature_scores = mutual_info_classif(X_vec, Ytrain)\n",
    "\n",
    "#for score, fname in sorted(zip(feature_scores, dv.get_feature_names_out()), reverse=True)[:10]:\n",
    "#    print(fname, score)\n",
    "    \n",
    "#from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "#pipeline = make_pipeline(\n",
    "#        DictVectorizer(),\n",
    "#        SelectKBest(mutual_info_classif, k=100), # or SelectPercentile(...)\n",
    "#        DecisionTreeClassifier()\n",
    "#)\n",
    "#pipeline.fit(Xtrain_dicts, Ytrain)\n",
    "#accuracy_score(Ytest, pipeline.predict(Xtest_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('Lec2_adult_train.csv')\n",
    "\n",
    "n_cols = len(train_data.columns)\n",
    "Xtrain_dicts = train_data.iloc[:, :n_cols-1].to_dict('records')\n",
    "Ytrain = train_data.iloc[:, n_cols-1]\n",
    "\n",
    "test_data = pd.read_csv('Lec2_adult_test.csv')\n",
    "Xtest_dicts = test_data.iloc[:, :n_cols-1].to_dict('records')\n",
    "Ytest = test_data.iloc[:, n_cols-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might recall, the instances in this dataset consist of several features describing each individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 27,\n",
       " 'workclass': 'Private',\n",
       " 'education': 'Some-college',\n",
       " 'education-num': 10,\n",
       " 'marital-status': 'Divorced',\n",
       " 'occupation': 'Adm-clerical',\n",
       " 'relationship': 'Unmarried',\n",
       " 'race': 'White',\n",
       " 'sex': 'Female',\n",
       " 'capital-gain': 0,\n",
       " 'capital-loss': 0,\n",
       " 'hours-per-week': 44,\n",
       " 'native-country': 'United-States'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the training set into numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dv = DictVectorizer()\n",
    "dv.fit(Xtrain_dicts)\n",
    "\n",
    "X_vec = dv.transform(Xtrain_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first scoring function we'll investigate is called the [mutual information](https://en.wikipedia.org/wiki/Mutual_information). [Here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) is the description from scikit-learn about how this scoring function works.\n",
    "\n",
    "(To see the formula used to compute the mutual information score, see the [description](https://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html) in the book *Introduction to Information Retrieval* by Manning and Sch√ºtze.)\n",
    "\n",
    "We apply the scoring function to all the features, and we then print the top 10 high-scoring features. Please refer back to the perceptron example in the previous lecture for an explanation about the step where we sort the features by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital-status=Married-civ-spouse 0.10543223425355985\n",
      "capital-gain 0.083382372123436\n",
      "relationship=Husband 0.08087684110742101\n",
      "age 0.0687725396789363\n",
      "education-num 0.064872227626807\n",
      "marital-status=Never-married 0.06195072410418583\n",
      "hours-per-week 0.0422833222022355\n",
      "relationship=Own-child 0.03821610420273137\n",
      "capital-loss 0.03698048451035268\n",
      "sex=Male 0.025765242400373284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "feature_scores = mutual_info_classif(X_vec, Ytrain)\n",
    "\n",
    "for score, fname in sorted(zip(feature_scores, dv.get_feature_names_out()), reverse=True)[:10]:\n",
    "    print(fname, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second scoring function uses the so-called $F$-statistic in an [ANOVA test](https://en.wikipedia.org/wiki/Analysis_of_variance).\n",
    "\n",
    "As you can see, there is an overlap between the top-10 list produced by this scorer and the previous list, but they are not identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital-status=Married-civ-spouse 8025.8420615949835\n",
      "relationship=Husband 6240.018276214241\n",
      "education-num 4120.095779707474\n",
      "marital-status=Never-married 3674.2001465697413\n",
      "age 1886.7073137161203\n",
      "hours-per-week 1813.3862822161334\n",
      "relationship=Own-child 1794.1574893573925\n",
      "capital-gain 1709.150063743795\n",
      "sex=Female 1593.1079074467164\n",
      "sex=Male 1593.1079074467073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "feature_scores = f_classif(X_vec, Ytrain)[0]\n",
    "\n",
    "for score, fname in sorted(zip(feature_scores, dv.get_feature_names_out()), reverse=True)[:10]:\n",
    "    print(fname, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another feature scoring function. It is based on the well-known [$\\chi^2$ statistical test](https://en.wikipedia.org/wiki/Chi-squared_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-gain 82192467.14154437\n",
      "capital-loss 1372145.890201465\n",
      "age 8600.61182155558\n",
      "hours-per-week 6476.4089959321245\n",
      "marital-status=Married-civ-spouse 3477.5158774537117\n",
      "relationship=Husband 3114.94154602898\n",
      "education-num 2401.4217771976464\n",
      "marital-status=Never-married 2218.5219765707857\n",
      "relationship=Own-child 1435.873016044718\n",
      "occupation=Exec-managerial 1315.4826322279757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "feature_scores = chi2(X_vec, Ytrain)[0]\n",
    "\n",
    "for score, fname in sorted(zip(feature_scores, dv.get_feature_names_out()), reverse=True)[:10]:\n",
    "    print(fname, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice when we'd like to use feature selection in scikit-learn, we just plug a selector into our pipeline. `SelectKBest` and `SelectPercentile` are the most common selectors. They use a feature scoring function (such as the ones above) to rank the features; by default, the `f_classif` scoring function is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166574534733738"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "        DictVectorizer(),\n",
    "        SelectKBest(k=100), # or SelectPercentile(...)\n",
    "        DecisionTreeClassifier()\n",
    ")\n",
    "pipeline.fit(Xtrain_dicts, Ytrain)\n",
    "accuracy_score(Ytest, pipeline.predict(Xtest_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
